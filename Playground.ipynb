{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ater97/Threads/blob/master/Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eGV-J84c2w8p",
        "colab_type": "code",
        "outputId": "a98f1bb0-3c87-4ad2-d719-9502b5709a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from time import sleep\n",
        "\n",
        "def threaded_function(arg):\n",
        "  for i in range(arg):\n",
        "    print (\"running\")\n",
        "    sleep(1)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  thread = Thread(target = threaded_function, args = (10,))\n",
        "  thread.start()\n",
        "  thread.join()\n",
        "  print (\"thread finished...exiting\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running\n",
            "running\n",
            "running\n",
            "running\n",
            "running\n",
            "running\n",
            "running\n",
            "running\n",
            "running\n",
            "running\n",
            "thread finished...exiting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WcJ3sZNx_j5j",
        "colab_type": "code",
        "outputId": "c4b7729b-ceea-4017-db44-8dff12a38c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "def fun1(a,b):\n",
        "  time.sleep(1)\n",
        "  c = a + b\n",
        "  print(c)\n",
        "\n",
        "thread1 = threading.Thread(target = fun1, args =(12,10))\n",
        "thread1.start()\n",
        "\n",
        "thread2 = threading.Thread(target = fun1, args =(10,17))\n",
        "thread2.start()\n",
        "\n",
        "thread1.join()\n",
        "thread2.join()\n",
        "print(\"Total number of threads \",threading.activeCount())\n",
        "print(\"List of threads \",threading.enumerate())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n",
            "27\n",
            "Total number of threads  5\n",
            "List of threads  [<_MainThread(MainThread, started 139770782795648)>, <Thread(Thread-2, started daemon 139770631231232)>, <Heartbeat(Thread-3, started daemon 139770622838528)>, <HistorySavingThread(IPythonHistorySavingThread, started 139770597660416)>, <ParentPollerUnix(Thread-1, started daemon 139770540218112)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_x5jak_DDFE7",
        "colab_type": "code",
        "outputId": "5c83aab5-06d2-4715-c9ce-393eb5a8b872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "#number2\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "# Supress warnig\n",
        "import os \n",
        "os.environ['TF_CPP_MiN_LOG_LEVEL']='2'\n",
        "\n",
        "# Number of classes \n",
        "num_classes = 10\n",
        "\n",
        "# Sizes of batch and # of epochs of data\n",
        "batch_size = 128\n",
        "epochs = 24\n",
        "\n",
        "# Input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# data, shuffled and split between and test sets\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Deal with format issues between different backends\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "  input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "  input_shape = (img_rows, img_cols, 1)\n",
        "  \n",
        "# Type convert and scale the test and trainig data  \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /=255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "#convolution\n",
        "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#categori...\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Define compile to minimize categoricl loss, use ada delta optimized\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "             optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "\n",
        "#Train model and test\n",
        "import threading\n",
        "import time\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "from multiprocessing import Process\n",
        "\n",
        "def train(i):\n",
        "  print(\"Thread number\" + str(i))\n",
        "  hist = model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=1,\n",
        "                verbose=1,\n",
        "                validation_data=(x_test, y_test))\n",
        "\n",
        "Pros =[]\n",
        "#epochs\n",
        "print(\"Thread started\")\n",
        "for i in range(0,3):\n",
        "  p = Process(target=train, args=(i,))\n",
        "  Pros.append(p)\n",
        "  p.start()\n",
        "\n",
        "for t in Pros:\n",
        "  t.join()\n",
        "  print(\"Thread joined\")\n",
        "print(\"Threads done\")\n",
        "\n",
        "\n",
        "#print(\"thread1\")\n",
        "#pool = ThreadPool(3)\n",
        "#result = pool.map(train(),range(3))\n",
        "#pool.close()\n",
        "#pool.join()\n",
        "\n",
        "# Evaluate the model with the test data to get the scores on \"real\" data.\n",
        "score=model.evaluate(x_test,y_test,verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Tets accuracy',score[1])\n",
        "\n",
        "\n",
        "#  Plot data to see relationships in training and validation data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "epoch_list = list(range(1, len(hist.history['acc']) + 1)) #values for x axis\n",
        "plt.plot(epoch_list, hist.history['acc'], epoch_list, hist.history['val_acc'])\n",
        "plt.legend(('Training Accuracy', 'Validation Accuracy'))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread started\n",
            "Thread number0\n",
            "Thread number1\n",
            "Thread number2\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TlU58ylMAylB",
        "colab_type": "code",
        "outputId": "17a82277-f2cf-4a08-e6f8-cad5b6b1c855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1316
        }
      },
      "cell_type": "code",
      "source": [
        "#number1\n",
        "import threading\n",
        "import time\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "# Supress warnig\n",
        "import os \n",
        "os.environ['TF_CPP_MiN_LOG_LEVEL']='2'\n",
        "\n",
        "# Number of classes \n",
        "num_classes = 10\n",
        "\n",
        "# Sizes of batch and # of epochs of data\n",
        "batch_size = 128\n",
        "epochs = 24\n",
        "\n",
        "# Input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "print(\"shuffled and split\")\n",
        "# data, shuffled and split between and test sets\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "print(\"format backends\")\n",
        "# Deal with format issues between different backends\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "  input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "  input_shape = (img_rows, img_cols, 1)\n",
        "  \n",
        "# Type convert and scale the test and trainig data  \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /=255\n",
        "\n",
        "print(\"convert class vectors\")\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "print(\"convolutions\")\n",
        "#convolution\n",
        "def conv1():\n",
        "  model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "             optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "  \n",
        "def conv2():\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "             optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "  \n",
        "def conv3():\n",
        "  model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "             optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "  \n",
        "def conv4():\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "             optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "#categori...\n",
        "def conv5():\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  # Define compile to minimize categoricl loss, use ada delta optimized\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "             optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "    \n",
        "#threads\n",
        "thread1 = threading.Thread(target = conv1())\n",
        "thread1.start()\n",
        "thread2 = threading.Thread(target = conv2())\n",
        "thread2.start()\n",
        "thread3 = threading.Thread(target = conv3())\n",
        "thread3.start()\n",
        "thread4 = threading.Thread(target = conv4())\n",
        "thread4.start()\n",
        "\n",
        "\n",
        "\n",
        "thread5 = threading.Thread(target = conv5)\n",
        "thread5.start()  \n",
        "\n",
        "thread1.join()\n",
        "thread2.join()\n",
        "thread3.join()\n",
        "thread4.join()\n",
        "thread5.join()\n",
        "\n",
        "#Train model and test\n",
        "hist = model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                verbose=1,\n",
        "                validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model with the test data to get the scores on \"real\" data.\n",
        "score=model.evaluate(x_test,y_test,verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Tets accuracy',score[1])\n",
        "\n",
        "\n",
        "#  Plot data to see relationships in training and validation data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "epoch_list = list(range(1, len(hist.history['acc']) + 1)) #values for x axis\n",
        "plt.plot(epoch_list, hist.history['acc'], epoch_list, hist.history['val_acc'])\n",
        "plt.legend(('Training Accuracy', 'Validation Accuracy'))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shuffled and split\n",
            "format backends\n",
            "convert class vectors\n",
            "convolutions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-19:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-10-8f57ebbce249>\", line 78, in conv5\n",
            "    model.add(Flatten())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\", line 181, in add\n",
            "    output_tensor = layer(self.outputs[0])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 457, in __call__\n",
            "    output = self.call(inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\", line 515, in call\n",
            "    return K.batch_flatten(inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2217, in batch_flatten\n",
            "    x = tf.reshape(x, tf.stack([-1, prod(shape(x)[1:])]))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 1307, in prod\n",
            "    return tf.reduce_prod(x, axis, keepdims)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1765, in reduce_prod_v1\n",
            "    return reduce_prod(input_tensor, axis, keepdims, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n",
            "    return target(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1722, in reduce_prod\n",
            "    name=name))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 6239, in prod\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 350, in _apply_op_helper\n",
            "    g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5713, in _get_graph_from_inputs\n",
            "    _assert_same_graph(original_graph_element, graph_element)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5649, in _assert_same_graph\n",
            "    original_item))\n",
            "ValueError: Tensor(\"flatten_1/Const:0\", shape=(1,), dtype=int32) must be from the same graph as Tensor(\"strided_slice:0\", shape=(3,), dtype=int32).\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8f57ebbce249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# Evaluate the model with the test data to get the scores on \"real\" data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected max_pooling2d_6 to have 4 dimensions, but got array with shape (60000, 10)"
          ]
        }
      ]
    }
  ]
}